from playwright.sync_api import sync_playwright
import json
from bs4 import BeautifulSoup
import json
import openai
from playwright.async_api import async_playwright
import re
import asyncio
import os
from flask import jsonify
from models.detail_content_craw import DetailContentCraw
from models.history_craw_class import HistoryCraw
from models.screnario_craw_class import *
from database import *
from bson import ObjectId
from datetime import datetime
import pytz

def crawl_data_by_html(url, content):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        # Truy c·∫≠p trang
        page.goto(url)
        page.wait_for_timeout(3000)
        # ƒê·ªçc d·ªØ li·ªáu JSON t·ª´ extract_with_gptkey()
        json_rs = json.loads(extract_with_gptkey(content))

        # L·∫•y t·∫•t c·∫£ c√°c field ƒë·ªông t·ª´ JSON
        # data_fields = json_rs["data"].keys()
        data_fields = [key for key, value in json_rs["data"].items() if isinstance(value, dict) and ("value" in value or "selector" in value)]

        list_json = []

        # Duy·ªát qua t·ª´ng field ƒë·ªông
        elements_dict = {}
        for field in data_fields:
            selector = json_rs["data"][field]["selector"]  # L·∫•y selector t·ª´ JSON
            elements = page.locator(selector).all()  # L·∫•y danh s√°ch ph·∫ßn t·ª≠
            elements_dict[field] = elements

        # Gh√©p c√°c d·ªØ li·ªáu th√†nh t·ª´ng b·∫£n ghi
        max_length = max(len(elements) for elements in elements_dict.values())  # T√¨m s·ªë l∆∞·ª£ng l·ªõn nh·∫•t
        
        for i in range(max_length):  
            json_data = {}
            for field in data_fields:
                elements = elements_dict[field]
                if i < len(elements):  # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu hay kh√¥ng
                    if field == "link":  # N·∫øu l√† link th√¨ l·∫•y thu·ªôc t√≠nh href,...
                        json_data[field] = elements[i].get_attribute("href") or \
                                           elements[i].get_attribute("data-href") or \
                                           elements[i].get_attribute("data-url")
                    elif field == "image":  # N·∫øu l√† image th√¨ l·∫•y src,...
                        json_data[field] = elements[i].get_attribute("src") or \
                                           elements[i].get_attribute("href") or \
                                           elements[i].get_attribute("srcset") or \
                                           elements[i].get_attribute("data-src") or \
                                           elements[i].get_attribute("data-url") or \
                                           elements[i].get_attribute("style")
                    else:  # M·∫∑c ƒë·ªãnh l·∫•y n·ªôi dung text
                        json_data[field] = elements[i].text_content().strip()
                else:
                    json_data[field] = None  # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, g√°n None
            list_json.append(json_data)

        browser.close()
        result_json = {
            "column": list(data_fields),
            "data": list_json
        }

        # Ghi v√†o file JSON
        output_filename = "output_with_playwright.json"
        with open(output_filename, "w", encoding="utf-8") as json_file:
            json.dump(list_json, json_file, ensure_ascii=False, indent=4)

        print(f"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ghi v√†o {output_filename}")
        
        return result_json

def read_file_content(filename):
    with open(filename, "r", encoding="utf-8") as file:
        return file.read()
    
def read_json_file_content(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return json.load(file)


    
def extract_with_gptkey(content):
    # html = read_file_content("div_content.txt")
    soup = BeautifulSoup(content, "html.parser")
    cleaned_html = soup.prettify()
    openai.api_key = os.getenv("OPENAI_API_KEY")
    chat_completion = openai.chat.completions.create(
        messages=[
            {"role": "user", "content": f"B·∫°n l√† m·ªôt AI gi√∫p ph√¢n t√≠ch HTML v√† tr√≠ch xu·∫•t d·ªØ li·ªáu quan tr·ªçng.\
                ƒê√¢y l√† n·ªôi dung HTML:\n{cleaned_html}\nH√£y tr·∫£ v·ªÅ Json g·ªìm c√°c n·ªôi dung quan tr·ªçng trong ƒëo·∫°n HTML tr√™n,\
                    t√¥i ch·ªâ c·∫ßn k·∫øt qu·∫£ json, kh√¥ng c·∫ßn gi·∫£i th√≠ch v√† c·∫•u tr√∫c c·ªßa nh·ªØng gi√° tr·ªã tr·∫£ v·ªÅ lu√¥n c·ªë ƒë·ªãnh v√† ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ sau:\
                        value l√† gi√° tr·ªã c·ªßa n·ªôi dung t√¨m ƒë∆∞·ª£c v√† selector l√† className d·∫´n t·ªõi th·∫ª ƒë√≥\
                            ho·∫∑c m·ªôt gi√° tr·ªã CSS selector n√†o ƒë√≥ d√πng ƒë·ªÉ ƒë·ªãnh nghƒ©a th·∫ª, c√†ng ch√≠nh x√°c c√†ng t·ªët,\
                                b·ªçc selector v√† value l√† t√™n c·ªßa thu·ªôc t√≠nh quan tr·ªçng ƒë√≥ v√† ƒë·∫∑t t√™n cho thu·ªôc t√≠nh ƒë√≥ sao cho c√≥ √Ω nghƒ©a\
                                    (Ngo·∫°i tr·ª´ c√°c tr∆∞·ªùng h·ª£p sau v·ªõi thu·ªôc t√≠nh l√† ·∫£nh th√¨ mang t√™n image, link th√¨ c√≥ t√™n l√† link), \
                                        v√† cu·ªëi c√πng t·∫•t c·∫£ c√°c thu·ªôc t√≠nh t√¨m ƒë∆∞·ª£c th√¨ s·∫Ω ƒë∆∞·ª£c b·ªçc v√†o 1 object l√† data"}
        ],
        model="gpt-4o-mini",
        temperature=0.7
    )    
    extracted_data = chat_completion.model_dump()["choices"][0]["message"]["content"]
    cleaned_json = re.sub(r"^```json\n|\n```$", "", extracted_data)

    # Chuy·ªÉn th√†nh JSON object
    data = json.loads(cleaned_json)
    
    # # Xu·∫•t JSON
    output_filename = "output_with_gpt.json"
    # with open(output_filename, "w", encoding="utf-8") as json_file:
    #     json.dump(data, json_file, ensure_ascii=False, indent=4)
        
    with open(output_filename, "w", encoding="utf-8") as file:
        json.dump(data, file, indent=2, ensure_ascii=False)    
    print("ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o file output_with_gpt.json")
    return json.dumps(data, indent=2, ensure_ascii=False)
    # # K·∫øt qu·∫£ JSON
    # extracted_data = chat_completion["choices"][0]["message"]["content"]
    # print(extracted_data)
        

    
    
async def fetch_with_playwright(url: str):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, slow_mo=50)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        # Ch·∫∑n qu·∫£ng c√°o, tracking
        async def block_ads(route):
            blocked_domains = ["ads", "analytics", "doubleclick", "googlesyndication"]
            if any(domain in route.request.url for domain in blocked_domains):
                await route.abort()
            else:
                await route.continue_()

        await page.route("**/*", block_ads)

        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=20000)  # Gi·∫£m timeout 20s
            # T·∫£i ·∫£nh khi load trang web 
            await page.evaluate('''() => {
                document.querySelectorAll('img[loading="lazy"]').forEach(img => {
                    if (img.dataset.src) img.src = img.dataset.src;
                    else if (img.dataset.srcset) img.src = img.dataset.srcset;
                    img.loading = "eager";  // ƒê·ªïi sang ch·∫ø ƒë·ªô t·∫£i ngay l·∫≠p t·ª©c
                });
            }''')
            await page.evaluate('''() => {
                document.querySelectorAll('img').forEach(img => {
                    if (img.dataset.src) {
                        img.src = img.dataset.src;    
                    } else if (img.dataset.srcset) {
                        img.src = img.dataset.srcset;
                    } 
                });
                document.querySelectorAll('video').forEach(video => {
                    video.querySelectorAll('source').forEach(source => {
                        if (source.dataset.srcVideo) {
                            source.src = source.dataset.srcVideo;
                        }
                    });
                    video.load();  // Load l·∫°i video sau khi thay src
                });
            }''')
            # page.on("requestfinished", lambda req: print(f"üì• Loaded: {req.url}"))
            # page.on("requestfailed", lambda req: print(f"‚ùå Failed: {req.url}"))
            await asyncio.sleep(1)
        except Exception as e:
            print(f"L·ªói khi t·∫£i trang: {e}")
            await browser.close()
            return None        
        # Cu·ªôn xu·ªëng ƒë·ªÉ t·∫£i th√™m n·ªôi dung
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await page.evaluate('''() => {
            document.querySelectorAll('img[loading="lazy"]').forEach(img => {
                if (img.dataset.src) img.src = img.dataset.src;
                else if (img.dataset.srcset) img.src = img.dataset.srcset;
                img.loading = "eager";  // ƒê·ªïi sang ch·∫ø ƒë·ªô t·∫£i ngay l·∫≠p t·ª©c
            });
        }''')
        await page.evaluate('''() => {
            document.querySelectorAll('img').forEach(img => {
                if (img.dataset.src) {
                    img.src = img.dataset.src;    
                } else if (img.dataset.srcset) {
                    img.src = img.dataset.srcset;
                } 
            });
            document.querySelectorAll('video').forEach(video => {
                //if (video.dataset.srcImage) {
                //    video.poster = video.dataset.srcImage;
                //}
                video.querySelectorAll('source').forEach(source => {
                    if (source.dataset.srcVideo) {
                        source.src = source.dataset.srcVideo;
                    }
                });
                video.load();  // Load l·∫°i video sau khi thay src
            });
        }''')
        
        # page.on("requestfinished", lambda req: print(f"üì• Loaded: {req.url}"))
        # page.on("requestfailed", lambda req: print(f"‚ùå Failed: {req.url}"))
        await asyncio.sleep(1)  # ƒê·ª£i trang t·∫£i

        # L·∫•y n·ªôi dung HTML
        html_content = await page.content()
        await browser.close()

        return html_content
    
    
def create_scenario_craw(scenaio):
    print(scenaio)
    
    scenaio_create =  ScenarioCraw()
    scenaio_create.__dict__ = scenaio.__dict__.copy()
    
    scrapedData = scenaio.scrapedData   
    new_scenaio = scenario_scraping.insert_one(scenaio_create.model_dump())
    if scrapedData is not None: 
        history = HistoryCraw(
            scenarioId=str(new_scenaio.inserted_id),
            timeScraped=datetime.now()
        )
        new_history_scenario = history_scraped.insert_one(history.model_dump())
        data_craw = DetailContentCraw(
            historyScrapedId=str(new_history_scenario.inserted_id),
            data=scrapedData
        )
        detail_data_scraped.insert_one(data_craw.model_dump())
    return {"id": str(new_scenaio.inserted_id)}

def update_scenario_craw(id, scenaio: ScenarioCraw):
    # Ki·ªÉm tra ID c√≥ h·ª£p l·ªá kh√¥ng
    if not ObjectId.is_valid(id):
        return {"status": "error", "message": "Invalid ID format"}, 400

    # Chuy·ªÉn ƒë·ªïi ID sang ObjectId
    obj_id = ObjectId(id)
    updated_record = scenario_scraping.find_one_and_update(
        {"_id": obj_id}, 
        {"$set": scenaio.to_dict()}, 
        return_document=True  # Tr·∫£ v·ªÅ b·∫£n ghi sau khi c·∫≠p nh·∫≠t
    )
    
    updated_record["_id"] = str(updated_record["_id"])
    

    if updated_record:
        return {"status": "success", "message": "Record updated successfully", "data": updated_record}, 200
    else:
        return {"status": "error", "message": "Record not found"}, 404


def delete_scenario_craw(id):
    # Ki·ªÉm tra ID c√≥ h·ª£p l·ªá kh√¥ng
    if not ObjectId.is_valid(id):
        return {"status": "error", "message": "Invalid ID format"}, 400

    # Chuy·ªÉn ƒë·ªïi ID sang ObjectId
    obj_id = ObjectId(id)
    
    existing_record = scenario_scraping.find_one({"_id": obj_id})
    if not existing_record:
        return {"status": "error", "message": "Record not found"}, 404

    # Th·ª±c hi·ªán x√≥a trong MongoDB
    scenario_scraping.delete_one({"_id": obj_id})
    return {"status": "success", "message": "Record deleted successfully"}, 200
    
def filter_Scenario(page, size):
    skip = (page - 1) * size

    # L·∫•y d·ªØ li·ªáu t·ª´ MongoDB
    scenarios = list(scenario_scraping.find().skip(skip).limit(size))

    # Chuy·ªÉn ƒë·ªïi ObjectId th√†nh string ƒë·ªÉ tr√°nh l·ªói
    for scenario in scenarios:
        scenario["_id"] = str(scenario["_id"])

    total_records = scenario_scraping.count_documents({})
    total_pages = (total_records + size - 1) // size

    return {
        "data": scenarios,
        "pagination": {
            "current_page": page,
            "total_pages": total_pages,
            "total_records": total_records,
            "page": page
        }
    }